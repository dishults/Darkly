https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/01-Information_Gathering/03-Review_Webserver_Metafiles_for_Information_Leakage

By checking robots.txt file we can see two disallowed folders /whatever and /.hidden.
In the second one we find lots of folders with README files.
Just make a crawler that reads those files and saves unique messages.
In one of those messages you'll find the flag.
So, after 1,5 min (if you treat links in reversed order) crawler returns:

Non ce n'est toujours pas bon ...

Demande Ã  ton voisin du dessus  

Demande Ã  ton voisin de gauche  

99dde1d35d1fdd283924d84e6d9f1d820

Tu veux de l'aide ? Moi aussi !  

Demande Ã  ton voisin du dessous 

Toujours pas tu vas craquer non ?

Demande Ã  ton voisin de droite 


FIX

Since Robots.txt directives may not be supported by all search engines and
    a robotted page can still be indexed if linked to from other sites, so
    it's not a valid technique to prevent access to private resources.
So better do:
- don't list private resources at all in robots.txt
- password protect those private resources
    and you might even require users to retype it on every access.
    Or at least, check on the server if the user is authenticated
    and has the right priveleges to access it and
    don't rely on the client side or robots.txt file.

https://developers.google.com/search/docs/advanced/robots/intro#understand-the-limitations-of-robots.txt
https://developers.google.com/search/docs/advanced/robots/robots-faq#h13